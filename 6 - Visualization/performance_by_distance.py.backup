"""
Performance by Distance Analysis
Analyzes REAL model performance as a function of simulated drone distance (SNR categories).

This script:
1. Loads trained models (CNN, RNN, CRNN)
2. Tests them on each augmentation category separately
3. Generates real performance metrics by distance/SNR
4. Compares how well each model handles different drone distances

Features:
- Real accuracy by SNR category
- Performance degradation analysis
- Per-model robustness comparison
- Confusion matrices by distance
"""

import os
import sys
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pandas as pd
from collections import defaultdict
import librosa
import math
import warnings

# Suppress warnings
warnings.filterwarnings('ignore', message='n_fft=.*is too large for input signal of length=.*')

# Import project config
sys.path.insert(0, str(Path(__file__).parent.parent))
import config

# Import ML libraries
try:
    from tensorflow import keras
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
except ImportError:
    print("[ERROR] TensorFlow/scikit-learn not installed")
    sys.exit(1)

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 10

# Audio processing parameters
SAMPLE_RATE = 22050
DURATION = 10
SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION


def load_models():
    """Load trained models."""
    models = {}
    
    model_paths = {
        'CNN': config.CNN_MODEL_PATH,
        'RNN': config.RNN_MODEL_PATH,
        'CRNN': config.CRNN_MODEL_PATH
    }
    
    for name, path in model_paths.items():
        if path.exists():
            try:
                models[name] = keras.models.load_model(path)
                print(f"[OK] Loaded {name} model")
            except Exception as e:
                print(f"[WARNING] Failed to load {name}: {e}")
        else:
            print(f"[WARNING] Model not found: {path}")
    
    return models


def preprocess_audio_for_mel(file_path, num_segments=10):
    """Preprocess audio file to Mel spectrogram format."""
    n_mels = 90
    n_fft = 2048
    hop_length = 512
    
    # Load audio
    signal, sr = librosa.load(file_path, sr=SAMPLE_RATE)
    
    num_samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)
    expected_num_vectors = math.ceil(num_samples_per_segment / hop_length)
    
    mels = []
    
    for s in range(num_segments):
        start_sample = num_samples_per_segment * s
        finish_sample = start_sample + num_samples_per_segment
        
        segment = signal[start_sample:finish_sample]
        
        # Pad if necessary
        if len(segment) < num_samples_per_segment:
            segment = np.pad(segment, (0, num_samples_per_segment - len(segment)), mode='constant')
        
        mel = librosa.feature.melspectrogram(y=segment, sr=sr, n_fft=n_fft, 
                                            n_mels=n_mels, hop_length=hop_length)
        db_mel = librosa.power_to_db(mel)
        mel_segment = db_mel.T
        
        if len(mel_segment) == expected_num_vectors:
            mels.append(mel_segment)
    
    return np.array(mels)


def preprocess_audio_for_mfcc(file_path, num_segments=10):
    """Preprocess audio file to MFCC format."""
    n_mfcc = 13
    n_fft = 2048
    hop_length = 512
    
    # Load audio
    signal, sr = librosa.load(file_path, sr=SAMPLE_RATE)
    
    num_samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)
    expected_num_vectors = math.ceil(num_samples_per_segment / hop_length)
    
    mfccs = []
    
    for s in range(num_segments):
        start_sample = num_samples_per_segment * s
        finish_sample = start_sample + num_samples_per_segment
        
        segment = signal[start_sample:finish_sample]
        
        # Pad if necessary
        if len(segment) < num_samples_per_segment:
            segment = np.pad(segment, (0, num_samples_per_segment - len(segment)), mode='constant')
        
        mfcc = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=n_mfcc, 
                                    n_fft=n_fft, hop_length=hop_length)
        mfcc_segment = mfcc.T
        
        if len(mfcc_segment) == expected_num_vectors:
            mfccs.append(mfcc_segment)
    
    return np.array(mfccs)


def get_category_files():
    """Get files organized by SNR category."""
    combined_path = config.PROJECT_ROOT / "0 - DADS dataset extraction" / "dataset_combined"
    
    if not combined_path.exists():
        print("[ERROR] Combined dataset not found")
        return None
    
    categories = {
        'Original (Clean)': [],
        '500m (-32dB)': [],
        '350m (-27dB)': [],
        '200m (-20dB)': [],
        '100m (-10dB)': [],
        '50m (0dB)': []
    }
    
    # Map filename patterns to categories - search in both 0/ and 1/ subdirectories
    for subfolder in ['0', '1']:
        subfolder_path = combined_path / subfolder
        if not subfolder_path.exists():
            continue
            
        for file_path in subfolder_path.glob('*.wav'):
            filename = file_path.name
            
            if 'drone_500m' in filename:
                categories['500m (-32dB)'].append(file_path)
            elif 'drone_350m' in filename:
                categories['350m (-27dB)'].append(file_path)
            elif 'drone_200m' in filename:
                categories['200m (-20dB)'].append(file_path)
            elif 'drone_100m' in filename:
                categories['100m (-10dB)'].append(file_path)
            elif 'drone_50m' in filename:
                categories['50m (0dB)'].append(file_path)
            elif 'orig_' in filename:
                categories['Original (Clean)'].append(file_path)
    
    # Filter out empty categories
    categories = {k: v for k, v in categories.items() if v}
    
    return categories


def evaluate_model_on_category(model, model_name, files, feature_type='mel'):
    """Evaluate a model on a specific category of files."""
    if not files:
        return None
    
    print(f"  Evaluating {len(files)} files...", end=' ')
    
    predictions = []
    labels = []  # All are drones (label=1) in this case
    
    preprocess_func = preprocess_audio_for_mel if feature_type == 'mel' else preprocess_audio_for_mfcc
    
    for file_path in files:
        try:
            # Extract true label from parent directory name
            # Files are in dataset_combined/0/ (no-drone) or dataset_combined/1/ (drone)
            true_label = int(file_path.parent.name)
            
            # Preprocess
            features = preprocess_func(file_path)
            
            if len(features) == 0:
                continue
            
            # Reshape for model input
            # For CNN/CRNN (mel): (segments, time_steps, features) -> (segments, time_steps, features, 1)
            # For RNN (mfcc): (segments, time_steps, features) -> (segments, time_steps, features)
            if feature_type == 'mel':
                features = features[..., np.newaxis]  # Add channel dimension
            
            # Predict
            pred = model.predict(features, verbose=0)
            
            # Average predictions across segments
            avg_pred = np.mean(pred, axis=0)
            
            # Debug first few predictions
            if len(predictions) < 3:
                print(f"\n  [DEBUG] File: {file_path.name}, True label: {true_label}")
                print(f"  [DEBUG] avg_pred: {avg_pred}, pred_label: {np.argmax(avg_pred)}")
            
            # Handle different output formats (softmax with 2 outputs)
            pred_label = np.argmax(avg_pred)
            
            predictions.append(pred_label)
            labels.append(true_label)
            
        except Exception as e:
            print(f"\n[WARNING] Error processing {file_path.name}: {e}")
            continue
    
    if not predictions:
        print("No valid predictions")
        return None
    
    # Debug: Show prediction distribution
    pred_array = np.array(predictions)
    label_array = np.array(labels)
    print(f"\n  [DEBUG] Predictions: {np.sum(pred_array == 0)} no-drones, {np.sum(pred_array == 1)} drones")
    print(f"  [DEBUG] True labels: {np.sum(label_array == 0)} no-drones, {np.sum(label_array == 1)} drones")
    
    # Calculate metrics
    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, 
                                                                average='binary', 
                                                                zero_division=0)
    
    print(f"  Accuracy: {accuracy*100:.1f}%, Recall: {recall*100:.1f}%, Precision: {precision*100:.1f}%")
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'predictions': predictions,
        'labels': labels
    }


def test_models_by_distance():
    """Test all models on each distance category."""
    print("[1/2] Loading models...")
    models = load_models()
    
    if not models:
        print("[ERROR] No models loaded")
        return None, None
    
    print(f"\n[2/2] Getting category files...")
    categories = get_category_files()
    
    if not categories:
        print("[ERROR] No category files found")
        return None, None
    
    print(f"Found {len(categories)} categories\n")
    
    # Test each model on each category
    results = defaultdict(dict)
    
    for model_name, model in models.items():
        print(f"\nTesting {model_name}:")
        print("-" * 60)
        
        # All models use mel spectrograms in this project
        feature_type = 'mel'
        
        for cat_name, files in categories.items():
            print(f"{cat_name:20s} ({len(files):3d} files): ", end='')
            
            result = evaluate_model_on_category(model, model_name, files[:100], feature_type)  # Limit to 100 for speed
            
            if result:
                results[model_name][cat_name] = result
    
    return results, categories


def plot_performance_by_distance(results, categories):
    """Plot real model performance by distance category."""
    if not results:
        print("[WARNING] No results to plot")
        return None
    
    # Prepare data
    cat_order = ['500m (-32dB)', '350m (-27dB)', '200m (-20dB)', 
                 '100m (-10dB)', '50m (0dB)', 'Original (Clean)']
    cat_order = [c for c in cat_order if c in list(results.values())[0]]
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # Plot 1: Accuracy by category
    ax1 = axes[0, 0]
    x = np.arange(len(cat_order))
    width = 0.25
    
    colors = {'CNN': 'steelblue', 'RNN': 'coral', 'CRNN': 'lightgreen'}
    
    for i, (model_name, model_results) in enumerate(results.items()):
        accuracies = [model_results.get(cat, {}).get('accuracy', 0) * 100 
                     for cat in cat_order]
        
        bars = ax1.bar(x + i*width, accuracies, width, label=model_name, 
                      color=colors.get(model_name, 'gray'), alpha=0.8)
        
        # Add value labels
        for bar in bars:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}%',
                    ha='center', va='bottom', fontsize=8, fontweight='bold')
    
    ax1.set_xlabel('Detection Scenario', fontweight='bold', fontsize=11)
    ax1.set_ylabel('Accuracy (%)', fontweight='bold', fontsize=11)
    ax1.set_title('Model Accuracy by Drone Distance', fontsize=13, fontweight='bold')
    ax1.set_xticks(x + width)
    ax1.set_xticklabels(cat_order, rotation=20, ha='right', fontsize=9)
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    ax1.set_ylim([0, 105])
    
    # Plot 2: F1-Score by category
    ax2 = axes[0, 1]
    
    for i, (model_name, model_results) in enumerate(results.items()):
        f1_scores = [model_results.get(cat, {}).get('f1', 0) * 100 
                    for cat in cat_order]
        
        bars = ax2.bar(x + i*width, f1_scores, width, label=model_name, 
                      color=colors.get(model_name, 'gray'), alpha=0.8)
        
        # Add value labels
        for bar in bars:
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}%',
                    ha='center', va='bottom', fontsize=8, fontweight='bold')
    
    ax2.set_xlabel('Detection Scenario', fontweight='bold', fontsize=11)
    ax2.set_ylabel('F1-Score (%)', fontweight='bold', fontsize=11)
    ax2.set_title('Model F1-Score by Drone Distance', fontsize=13, fontweight='bold')
    ax2.set_xticks(x + width)
    ax2.set_xticklabels(cat_order, rotation=20, ha='right', fontsize=9)
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)
    ax2.set_ylim([0, 105])
    
    # Plot 3: Performance degradation curves
    ax3 = axes[1, 0]
    
    snr_values = [-15, -10, -5, 0, 5, 100]  # 100 for original
    cat_to_snr = {cat: snr for cat, snr in zip(cat_order, snr_values[:len(cat_order)])}
    
    for model_name, model_results in results.items():
        snrs = []
        accs = []
        
        for cat in cat_order:
            if cat in model_results:
                snrs.append(cat_to_snr.get(cat, 0))
                accs.append(model_results[cat]['accuracy'] * 100)
        
        ax3.plot(snrs, accs, marker='o', linewidth=2.5, markersize=10,
                label=model_name, color=colors.get(model_name, 'gray'), alpha=0.8)
    
    ax3.set_xlabel('SNR (dB) - Higher = Closer Drone', fontweight='bold', fontsize=11)
    ax3.set_ylabel('Accuracy (%)', fontweight='bold', fontsize=11)
    ax3.set_title('Performance vs. Drone Distance (Real Data)', fontsize=13, fontweight='bold')
    ax3.legend(fontsize=10)
    ax3.grid(alpha=0.3)
    ax3.set_ylim([0, 105])
    ax3.axhline(y=50, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Random Chance')
    
    # Add difficulty zones
    ax3.axvspan(-20, -10, alpha=0.1, color='red')
    ax3.axvspan(-10, -5, alpha=0.1, color='orange')
    ax3.axvspan(-5, 0, alpha=0.1, color='yellow')
    ax3.axvspan(0, 10, alpha=0.1, color='green')
    
    # Plot 4: Sample count by category
    ax4 = axes[1, 1]
    
    sample_counts = [len(categories[cat]) for cat in cat_order]
    
    difficulty_colors = ['#e74c3c', '#e67e22', '#f39c12', '#2ecc71', '#27ae60', '#3498db']
    bars = ax4.bar(x, sample_counts, color=difficulty_colors[:len(cat_order)], 
                   alpha=0.8, edgecolor='black', linewidth=1.5)
    
    for bar, count in zip(bars, sample_counts):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height,
                str(count),
                ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    ax4.set_xlabel('Detection Scenario', fontweight='bold', fontsize=11)
    ax4.set_ylabel('Number of Test Samples', fontweight='bold', fontsize=11)
    ax4.set_title('Test Sample Distribution', fontsize=13, fontweight='bold')
    ax4.set_xticks(x)
    ax4.set_xticklabels(cat_order, rotation=20, ha='right', fontsize=9)
    ax4.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    
    # Save
    output_dir = Path(__file__).parent / 'outputs'
    output_dir.mkdir(exist_ok=True)
    plt.savefig(output_dir / 'real_performance_by_distance.png', dpi=300, bbox_inches='tight')
    print(f"\n[OK] Saved: {output_dir / 'real_performance_by_distance.png'}")
    
    return fig


def generate_performance_table(results):
    """Generate detailed performance table by distance."""
    if not results:
        return None
    
    cat_order = ['500m (-32dB)', '350m (-27dB)', '200m (-20dB)', 
                 '100m (-10dB)', '50m (0dB)', 'Original (Clean)']
    
    table_data = []
    
    for cat in cat_order:
        row = {'Category': cat}
        
        for model_name, model_results in results.items():
            if cat in model_results:
                res = model_results[cat]
                row[f'{model_name} Acc (%)'] = f"{res['accuracy']*100:.2f}"
                row[f'{model_name} F1 (%)'] = f"{res['f1']*100:.2f}"
        
        if len(row) > 1:  # Has data
            table_data.append(row)
    
    df = pd.DataFrame(table_data)
    
    # Save
    output_dir = Path(__file__).parent / 'outputs'
    output_dir.mkdir(exist_ok=True)
    
    csv_path = output_dir / 'performance_by_distance.csv'
    df.to_csv(csv_path, index=False)
    print(f"[OK] Saved: {csv_path}")
    
    # Print table
    print("\n" + "="*100)
    print("REAL PERFORMANCE BY DISTANCE/SNR")
    print("="*100)
    print(df.to_string(index=False))
    print("="*100 + "\n")
    
    return df


def analyze_performance_by_distance():
    """
    Analyze performance by distance category using augmentation metadata.
    """
    # Load augmentation metadata
    metadata_path = config.PROJECT_ROOT / "0 - DADS dataset extraction" / "dataset_augmented" / "augmentation_metadata.json"
    
    if not metadata_path.exists():
        print("[WARNING] Augmentation metadata not found")
        return None
    
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
    
    # Extract category information
    categories_info = metadata.get('config', {}).get('augmented_categories', [])
    stats = metadata.get('statistics', {}).get('categories', {})
    
    # Create structured data
    distance_data = []
    
    for cat_info in categories_info:
        cat_name = cat_info['name']
        cat_stats = stats.get(cat_name, {})
        
        distance_data.append({
            'category': cat_name,
            'display_name': cat_info.get('description', cat_name),
            'snr_db': cat_info['snr_db'],
            'achieved_snr': cat_stats.get('avg_snr_db', cat_info['snr_db']),
            'num_samples': cat_stats.get('generated', 0),
            'num_bg_noises': cat_info['num_background_noises']
        })
    
    # Add original samples
    dataset_test_path = config.PROJECT_ROOT / "0 - DADS dataset extraction" / "dataset_test" / "1"
    if dataset_test_path.exists():
        orig_count = len(list(dataset_test_path.glob('*.wav')))
        distance_data.append({
            'category': 'original',
            'display_name': 'Original (Clean)',
            'snr_db': 100,  # Placeholder for "infinite" SNR
            'achieved_snr': 100,
            'num_samples': orig_count,
            'num_bg_noises': 0
        })
    
    return distance_data


def plot_difficulty_spectrum_simple(distance_data):
    """Plot the difficulty spectrum (simplified version)."""
    # Sort by SNR
    distance_data.sort(key=lambda x: x['snr_db'])
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    categories = [d['display_name'] for d in distance_data]
    samples = [d['num_samples'] for d in distance_data]
    
    colors = ['#e74c3c', '#e67e22', '#f39c12', '#2ecc71', '#27ae60', '#3498db']
    
    bars = ax.bar(range(len(categories)), samples, color=colors[:len(categories)], 
                  alpha=0.8, edgecolor='black', linewidth=1.5)
    
    for bar, count in zip(bars, samples):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               str(count),
               ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    ax.set_xlabel('Detection Scenario (Easier â†’)', fontweight='bold', fontsize=12)
    ax.set_ylabel('Number of Samples', fontweight='bold', fontsize=12)
    ax.set_title('Test Sample Distribution by Difficulty', fontsize=14, fontweight='bold')
    ax.set_xticks(range(len(categories)))
    ax.set_xticklabels(categories, rotation=20, ha='right')
    ax.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    
    output_dir = Path(__file__).parent / 'outputs'
    plt.savefig(output_dir / 'difficulty_spectrum.png', dpi=300, bbox_inches='tight')
    print(f"[OK] Saved: {output_dir / 'difficulty_spectrum.png'}")
    
    return fig


def main():
    """Run real performance analysis by distance."""
    print("="*80)
    print("REAL PERFORMANCE BY DISTANCE ANALYSIS")
    print("="*80 + "\n")
    
    print("Testing trained models on each SNR category...")
    print("This will take several minutes...\n")
    
    # Test models on each category
    print("[STEP 1/4] Testing models by distance category...")
    results, categories = test_models_by_distance()
    
    if not results:
        print("[ERROR] No results obtained")
        return 1
    
    # Generate performance table
    print("\n[STEP 2/4] Generating performance table...")
    generate_performance_table(results)
    
    # Plot results
    print("\n[STEP 3/4] Plotting performance by distance...")
    plot_performance_by_distance(results, categories)
    
    # Plot difficulty spectrum
    print("\n[STEP 4/4] Plotting difficulty spectrum...")
    distance_data = analyze_performance_by_distance()
    if distance_data:
        plot_difficulty_spectrum_simple(distance_data)
    
    print("\n" + "="*80)
    print("[SUCCESS] Real performance by distance analysis complete!")
    print("="*80)
    print(f"\nOutputs saved in: {Path(__file__).parent / 'outputs'}")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
