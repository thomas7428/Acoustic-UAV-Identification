# Visualization Suite

Ce dossier contient les scripts de visualisation et d'analyse pour le projet Acoustic UAV Identification.

## ğŸš€ Quick Start

### Usage RecommandÃ© (Simple et Rapide)

```bash
# Pipeline complet de visualisations
python run_visualizations.py

# Pipeline rapide (sans audio examples ni threshold calibration)
python quick_viz.py fast

# Seulement la comparaison de performances
python quick_viz.py performance

# Pipeline sans audio examples (plus rapide)
python run_visualizations.py --skip-audio
```

---

## ğŸ“‹ Scripts Principaux

### ğŸ¯ 1. `run_visualizations.py` â­ **RUNNER UNIFIÃ‰**
**Point d'entrÃ©e principal pour gÃ©nÃ©rer toutes les visualisations.**

Pipeline complet en 8 Ã©tapes:
1. Select Best Results (gÃ©nÃ¨re best_results_summary.json)
2. Performance Comparison (meilleurs thresholds)
3. Threshold Calibration Comparison
4. Model Comparison Plots
5. SNR Distribution Analysis
6. Dataset Composition Analysis
7. Modern Threshold Calibration
8. HTML Report Generation

**Options:**
```bash
python run_visualizations.py                    # Complet
python run_visualizations.py --skip-audio       # Sans audio examples
python run_visualizations.py --skip-threshold   # Sans threshold calibration
```

**Avantages:**
- âœ… Pipeline automatique complet
- âœ… Gestion d'erreurs robuste
- âœ… RÃ©sumÃ© clair des rÃ©sultats
- âœ… Inclut select_best_results automatiquement

---

### âš¡ 2. `quick_viz.py` - Lanceur de Presets
**Raccourcis pour les cas d'usage courants.**

**Presets disponibles:**
- `all` - Pipeline complet
- `fast` - Rapide (sans audio/threshold)
- `performance` - Seulement performances
- `no-audio` - Complet sans audio

**Usage:**
```bash
python quick_viz.py all           # Pipeline complet
python quick_viz.py fast          # Rapide
python quick_viz.py performance   # Juste performances
```

---

## ğŸ¨ Scripts de Visualisation Individuels
**Analyse de la composition et statistiques du dataset.**

Visualise la distribution des classes, sous-catÃ©gories et splits.

**GÃ©nÃ¨re :**
- `dataset_split_distribution.png` - Distribution par split
- `dataset_drone_distribution.png` - Drones par distance
- `dataset_ambient_distribution.png` - Ambients par type
- `dataset_summary.txt` - Rapport complet

**Usage :**
```bash
python modern_dataset_analysis.py
```

---

### 3. `modern_dataset_analysis.py`
**GÃ©nÃ¨re des exemples audio reprÃ©sentatifs avec visualisations.**

CrÃ©e une page HTML interactive avec lecteurs audio, waveforms et spectrogrammes.

**GÃ©nÃ¨re :**
- Fichiers WAV copiÃ©s
- Visualisations (waveform + spectrogramme)
- Page HTML avec lecteurs audio
- Dossier : `outputs/audio_examples/`

**Usage :**
```bash
python modern_audio_examples.py
# Ouvrir: outputs/audio_examples/index.html
```

---

### 4. `modern_audio_examples.py`
**Analyse systÃ©matique des thresholds de dÃ©cision.**

Recommande les thresholds optimaux par modÃ¨le pour diffÃ©rents critÃ¨res (F1, accuracy, Ã©quilibrage FP/FN).

**GÃ©nÃ¨re :**
- `threshold_calibration_{model}_{split}.png` - Courbes et recommandations
- `threshold_recommendations.json` - Thresholds optimaux
- `threshold_recommendations.txt` - Rapport lisible

**Usage :**
```bash
python modern_threshold_calibration.py
# NÃ©cessite plusieurs thresholds testÃ©s avec Universal_Perf_Tester.py
```

---

### 5. `modern_threshold_calibration.py`
python run_all_visualizations.py --skip-audio   # Sans audio examples
```

---

### 6. `threshold_calibration_comparison.py`
Analyse l'impact des thresholds sur les performances.

**GÃ©nÃ¨re:** `threshold_calibration_comparison.png`

---

### 7. `model_comparison_plots.py`
Comparaisons visuelles entre modÃ¨les.

**GÃ©nÃ¨re:** `model_performance_comparison.png`

---

### 8. `snr_distribution.py`
Analyse de la distribution SNR.

**GÃ©nÃ¨re:** `snr_distribution.png`

---

### 9. `generate_html_report.py`
GÃ©nÃ¨re un rapport HTML interactif avec toutes les visualisations.

**GÃ©nÃ¨re:**
- `performance_report.html`
- `images/` (dossier avec toutes les images)

**Note:** Utilise maintenant des chemins relatifs au lieu de base64 (~10x plus lÃ©ger).

---

## ğŸ“ Organisation

```
6 - Visualization/
â”œâ”€â”€ run_visualizations.py          â­ Runner unifiÃ© (PRINCIPAL)
â”œâ”€â”€ quick_viz.py                   ğŸš€ Presets rapides
â”œâ”€â”€ select_best_results.py         ğŸ¯ SÃ©lection best results
â”œâ”€â”€ performance_comparison_best.py ğŸ“Š Comparaison performances
â”œâ”€â”€ modern_dataset_analysis.py     ğŸ“ˆ Analyse dataset
â”œâ”€â”€ modern_audio_examples.py       ğŸµ Exemples audio
â”œâ”€â”€ modern_threshold_calibration.py ğŸšï¸ Calibration thresholds
â”œâ”€â”€ threshold_calibration_comparison.py
â”œâ”€â”€ model_comparison_plots.py
â”œâ”€â”€ snr_distribution.py
â”œâ”€â”€ generate_html_report.py        ğŸŒ Rapport HTML
â”œâ”€â”€ README.md                      ğŸ“– Documentation
â”œâ”€â”€ outputs/                       ğŸ’¾ RÃ©sultats
â”‚   â”œâ”€â”€ *.png
â”‚   â”œâ”€â”€ *.txt
â”‚   â”œâ”€â”€ *.json
â”‚   â”œâ”€â”€ performance_report.html
â”‚   â”œâ”€â”€ images/                    (pour le HTML report)
â”‚   â””â”€â”€ audio_examples/
â””â”€â”€ _deprecated/                   ğŸ—„ï¸ Scripts obsolÃ¨tes
    â”œâ”€â”€ _old_run_all_visualizations.py
    â”œâ”€â”€ _old_run_enhanced_visualizations.py
    â””â”€â”€ _deprecated_performance_comparison.py

```

---

## ğŸ”„ Migration depuis l'Ancien SystÃ¨me

### Si vous utilisiez `run_all_visualizations.py`:
```bash
# Ancien:
python run_all_visualizations.py

# Nouveau:
python run_visualizations.py
```

### Si vous utilisiez `run_enhanced_visualizations.py`:
```bash
# Ancien:
python run_enhanced_visualizations.py

# Nouveau:
python run_visualizations.py
```

### Si vous utilisiez `performance_comparison.py --all`:
```bash
# Ancien:
python performance_comparison.py --all

# Nouveau:
python quick_viz.py all
# ou
python run_visualizations.py
```

---

## ğŸš€ Workflow Rapide

### GÃ©nÃ©rer des rÃ©sultats de performance

```bash
# 1. Tester un modÃ¨le (gÃ©nÃ¨re JSON)
python "3 - Single Model Performance Calculation/Universal_Perf_Tester.py" \
    --model CNN --split test --threshold 0.5

# 2. Visualiser immÃ©diatement
cd "6 - Visualization"
python quick_viz.py all
```

### Analyse complÃ¨te

```bash
# GÃ©nÃ©rer toutes les visualisations
python run_all_visualizations.py

# RÃ©sultats dans outputs/
ls outputs/
```

### Comparaisons personnalisÃ©es

```bash
# Comparer CNN vs CRNN
python performance_comparison.py --models CNN CRNN --splits test

# Analyser l'impact du threshold
python performance_comparison.py --models CNN --thresholds 0.4 0.5 0.6 0.7
```

---

## ğŸ“– Documentation ComplÃ¨te

- **`README.md`** (ce fichier) - Vue d'ensemble et rÃ©fÃ©rence des scripts
- **`WORKFLOW.md`** - Guide dÃ©taillÃ© Ã©tape par Ã©tape avec cas d'usage
- **`archives/README.md`** - Information sur les scripts legacy

---

## ğŸ’¡ Tips

1. **Utiliser les JSON prÃ©calculÃ©s** : Tous les scripts modernes lisent depuis `config.PERFORMANCE_DIR`, donc testez vos modÃ¨les une fois avec `Universal_Perf_Tester.py`, puis visualisez Ã  volontÃ© sans recalcul.

2. **Presets rapides** : `quick_viz.py` offre des configurations prÃªtes Ã  l'emploi pour les cas d'usage courants.

3. **Filtrage intelligent** : `performance_comparison.py` peut combiner train/val/test ou comparer diffÃ©rents thresholds automatiquement.

4. **Timestamps automatiques** : Les fichiers JSON incluent un timestamp, donc plusieurs tests du mÃªme modÃ¨le ne s'Ã©crasent jamais.

5. **run_all_visualizations** : GÃ©nÃ¨re toutes les visualisations essentielles en une commande.

---

## ğŸ”§ DÃ©pannage

**"No JSON files found"**
- Lancer `Universal_Perf_Tester.py` d'abord pour gÃ©nÃ©rer les rÃ©sultats

**"No multi-threshold results"**
- Pour l'analyse de thresholds, tester avec plusieurs valeurs (0.4, 0.5, 0.6, etc.)

**Erreur d'import**
- VÃ©rifier que vous exÃ©cutez depuis le virtualenv : `.venv/bin/python`

---

## ğŸ“Š Outputs GÃ©nÃ©rÃ©s

Tous les rÃ©sultats sont sauvegardÃ©s dans `outputs/` :

**Visualisations PNG:**
- Performance globale et par classe
- Matrices de confusion
- Courbes par distance/type
- Impact des thresholds

**Rapports Texte:**
- `performance_summary.txt` - MÃ©triques dÃ©taillÃ©es
- `dataset_summary.txt` - Stats du dataset
- `threshold_recommendations.txt` - Thresholds optimaux

**DonnÃ©es JSON:**
- RÃ©sultats bruts pour post-traitement

**Pages HTML:**
- `audio_examples/index.html` - Exemples audio interactifs

---

**GÃ©nÃ¨re :**
- Spectre de difficultÃ© de dÃ©tection (trÃ¨s loin â†’ trÃ¨s proche)
- Courbes thÃ©oriques de performance vs distance
- Table d'analyse par catÃ©gorie de distance
- Distribution des Ã©chantillons par difficultÃ©

**Usage :**
```bash
python performance_by_distance.py
```

**Note :** Ce script gÃ©nÃ¨re des courbes thÃ©oriques. Pour des performances rÃ©elles par catÃ©gorie, il faudrait Ã©valuer les modÃ¨les sur un test set avec labels de catÃ©gorie SNR.

### 5. `run_all_visualizations.py`
Lance tous les scripts de visualisation en une seule commande.

**Usage :**
```bash
python run_all_visualizations.py
```

## Structure des sorties

Toutes les visualisations sont sauvegardÃ©es dans le dossier `outputs/` :

```
outputs/
â”œâ”€â”€ dataset_distribution.png
â”œâ”€â”€ snr_distribution.png
â”œâ”€â”€ audio_examples.png
â”œâ”€â”€ dataset_statistics.json
â”œâ”€â”€ training_curves.png
â”œâ”€â”€ confusion_matrices.png
â”œâ”€â”€ metrics_comparison.png
â”œâ”€â”€ performance_table.csv
â”œâ”€â”€ snr_performance.png
â”œâ”€â”€ augmentation_composition.png
â”œâ”€â”€ dataset_evolution.png
â”œâ”€â”€ difficulty_spectrum.png
â”œâ”€â”€ performance_vs_distance.png
â””â”€â”€ distance_analysis.csv
```

## DÃ©pendances

Les scripts nÃ©cessitent les bibliothÃ¨ques suivantes :
- matplotlib
- seaborn
- numpy
- pandas
- librosa
- scikit-learn

Ces dÃ©pendances sont normalement dÃ©jÃ  installÃ©es avec le projet principal.

## Ordre d'exÃ©cution recommandÃ©

1. **AprÃ¨s gÃ©nÃ©ration du dataset :** `dataset_analysis.py`
2. **AprÃ¨s entraÃ®nement des modÃ¨les :** `model_performance.py`
3. **Pour analyse complÃ¨te :** `augmentation_impact.py`
4. **Pour tout gÃ©nÃ©rer :** `run_all_visualizations.py`

## Notes

- Les scripts utilisent le fichier `config.py` Ã  la racine du projet pour les chemins
- Si un fichier de donnÃ©es est manquant, le script affiche un avertissement mais continue
- Les graphiques sont sauvegardÃ©s en haute rÃ©solution (300 DPI) pour publication
- Format des sorties : PNG pour les images, JSON/CSV pour les donnÃ©es

## Exemple de workflow complet

```bash
# 1. GÃ©nÃ©rer le dataset et entraÃ®ner les modÃ¨les
cd "../0 - DADS dataset extraction"
python master_setup.py --complete --max-per-class 500 --augment

cd "../1 - Preprocessing and Features Extraction"
python Mel_Preprocess_and_Feature_Extract.py
python MFCC_Preprocess_and_Feature_Extract.py

cd "../2 - Model Training"
python CNN_Trainer.py
python RNN_Trainer.py
python CRNN_Trainer.py

# 2. GÃ©nÃ©rer toutes les visualisations
cd "../6 - Visualization"
python run_all_visualizations.py
```

## Personnalisation

Pour personnaliser les visualisations, modifiez les paramÃ¨tres au dÃ©but de chaque script :
- Style matplotlib/seaborn
- Tailles des figures
- Couleurs
- Polices

## Support

En cas de problÃ¨me, vÃ©rifiez que :
1. Le dataset est bien gÃ©nÃ©rÃ© dans `0 - DADS dataset extraction/`
2. Les modÃ¨les sont entraÃ®nÃ©s et les rÃ©sultats sont dans `results/`
3. Les chemins dans `config.py` sont corrects
4. Toutes les dÃ©pendances sont installÃ©es
