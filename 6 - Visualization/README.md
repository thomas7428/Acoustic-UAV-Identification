# Visualization Suite

Ce dossier contient les scripts de visualisation et d'analyse pour le projet Acoustic UAV Identification.

## Scripts disponibles

## ğŸ¯ Scripts Modernes (RecommandÃ©s)

### ğŸ†• 1. `performance_comparison.py` â­ **PRINCIPAL**
**Script de visualisation des performances utilisant les rÃ©sultats JSON prÃ©calculÃ©s.**

Visualisation complÃ¨te Ã  partir des fichiers gÃ©nÃ©rÃ©s par `Universal_Perf_Tester.py`.

**Features :**
- Chargement depuis `config.PERFORMANCE_DIR`
- Comparaison multi-modÃ¨les, multi-splits, multi-thresholds
- MÃ©triques globales, par classe, et par sous-catÃ©gorie
- Matrices de confusion, courbes par distance/type
- Analyse de l'impact des thresholds
- Rapports texte dÃ©taillÃ©s

**Usage rapide :**
```bash
# Tous les rÃ©sultats disponibles
python performance_comparison.py --all

# Presets rapides
python quick_viz.py all                  # Tout visualiser
python quick_viz.py compare-models       # Comparaison standard
```

**Avantages :**
- âœ… InstantanÃ© (JSON prÃ©calculÃ©s)
- âœ… Config centralisÃ©e
- âœ… Filtres flexibles
- âœ… Pas de fallback

---

### 2. `modern_dataset_analysis.py`
**Analyse de la composition et statistiques du dataset.**

Visualise la distribution des classes, sous-catÃ©gories et splits.

**GÃ©nÃ¨re :**
- `dataset_split_distribution.png` - Distribution par split
- `dataset_drone_distribution.png` - Drones par distance
- `dataset_ambient_distribution.png` - Ambients par type
- `dataset_summary.txt` - Rapport complet

**Usage :**
```bash
python modern_dataset_analysis.py
```

---

### 3. `modern_audio_examples.py`
**GÃ©nÃ¨re des exemples audio reprÃ©sentatifs avec visualisations.**

CrÃ©e une page HTML interactive avec lecteurs audio, waveforms et spectrogrammes.

**GÃ©nÃ¨re :**
- Fichiers WAV copiÃ©s
- Visualisations (waveform + spectrogramme)
- Page HTML avec lecteurs audio
- Dossier : `outputs/audio_examples/`

**Usage :**
```bash
python modern_audio_examples.py
# Ouvrir: outputs/audio_examples/index.html
```

---

### 4. `modern_threshold_calibration.py`
**Analyse systÃ©matique des thresholds de dÃ©cision.**

Recommande les thresholds optimaux par modÃ¨le pour diffÃ©rents critÃ¨res (F1, accuracy, Ã©quilibrage FP/FN).

**GÃ©nÃ¨re :**
- `threshold_calibration_{model}_{split}.png` - Courbes et recommandations
- `threshold_recommendations.json` - Thresholds optimaux
- `threshold_recommendations.txt` - Rapport lisible

**Usage :**
```bash
python modern_threshold_calibration.py
# NÃ©cessite plusieurs thresholds testÃ©s avec Universal_Perf_Tester.py
```

---

### 5. `run_all_visualizations.py`
**Lance tous les scripts modernes en une seule commande.**

**Usage :**
```bash
python run_all_visualizations.py                # Tout gÃ©nÃ©rer
python run_all_visualizations.py --skip-audio   # Sans audio examples
```

---

### 6. `quick_viz.py`
**Launcher rapide avec presets pour performance_comparison.py.**

**Presets disponibles :**
- `all` - Tous les rÃ©sultats
- `compare-models` - Tous modÃ¨les sur test @ t=0.5
- `threshold-analysis` - CNN avec thresholds multiples
- `custom` - Arguments personnalisÃ©s

**Usage :**
```bash
python quick_viz.py all
python quick_viz.py compare-models
python quick_viz.py custom --models CNN --splits test val
```

---

## ğŸ“ Organisation

```
6 - Visualization/
â”œâ”€â”€ performance_comparison.py      â­ Principal
â”œâ”€â”€ quick_viz.py                   ğŸš€ Launcher
â”œâ”€â”€ modern_dataset_analysis.py     ğŸ“Š Dataset
â”œâ”€â”€ modern_audio_examples.py       ğŸµ Audio
â”œâ”€â”€ modern_threshold_calibration.py ğŸ¯ Thresholds
â”œâ”€â”€ run_all_visualizations.py      ğŸ”„ Tout exÃ©cuter
â”œâ”€â”€ README.md                      ğŸ“– Documentation
â”œâ”€â”€ WORKFLOW.md                    ğŸ“‹ Guide complet
â”œâ”€â”€ outputs/                       ğŸ’¾ RÃ©sultats
â”‚   â”œâ”€â”€ *.png
â”‚   â”œâ”€â”€ *.txt
â”‚   â””â”€â”€ audio_examples/
â””â”€â”€ archives/                      ğŸ—„ï¸ Scripts legacy
    â””â”€â”€ README.md

```

---

## ğŸ“š Scripts Legacy (Archives)

Les anciens scripts ont Ã©tÃ© dÃ©placÃ©s dans `archives/` :
- `audio_examples.py`
- `dataset_analysis.py`
- `model_performance.py`
- `threshold_calibration.py`
- `performance_by_distance.py`
- `augmentation_impact.py`

**âš ï¸ Ces scripts sont obsolÃ¨tes.** Utilisez les versions modernes ci-dessus.

Voir `archives/README.md` pour plus de dÃ©tails.

---

## ğŸš€ Workflow Rapide

### GÃ©nÃ©rer des rÃ©sultats de performance

```bash
# 1. Tester un modÃ¨le (gÃ©nÃ¨re JSON)
python "3 - Single Model Performance Calculation/Universal_Perf_Tester.py" \
    --model CNN --split test --threshold 0.5

# 2. Visualiser immÃ©diatement
cd "6 - Visualization"
python quick_viz.py all
```

### Analyse complÃ¨te

```bash
# GÃ©nÃ©rer toutes les visualisations
python run_all_visualizations.py

# RÃ©sultats dans outputs/
ls outputs/
```

### Comparaisons personnalisÃ©es

```bash
# Comparer CNN vs CRNN
python performance_comparison.py --models CNN CRNN --splits test

# Analyser l'impact du threshold
python performance_comparison.py --models CNN --thresholds 0.4 0.5 0.6 0.7
```

---

## ğŸ“– Documentation ComplÃ¨te

- **`README.md`** (ce fichier) - Vue d'ensemble et rÃ©fÃ©rence des scripts
- **`WORKFLOW.md`** - Guide dÃ©taillÃ© Ã©tape par Ã©tape avec cas d'usage
- **`archives/README.md`** - Information sur les scripts legacy

---

## ğŸ’¡ Tips

1. **Utiliser les JSON prÃ©calculÃ©s** : Tous les scripts modernes lisent depuis `config.PERFORMANCE_DIR`, donc testez vos modÃ¨les une fois avec `Universal_Perf_Tester.py`, puis visualisez Ã  volontÃ© sans recalcul.

2. **Presets rapides** : `quick_viz.py` offre des configurations prÃªtes Ã  l'emploi pour les cas d'usage courants.

3. **Filtrage intelligent** : `performance_comparison.py` peut combiner train/val/test ou comparer diffÃ©rents thresholds automatiquement.

4. **Timestamps automatiques** : Les fichiers JSON incluent un timestamp, donc plusieurs tests du mÃªme modÃ¨le ne s'Ã©crasent jamais.

5. **run_all_visualizations** : GÃ©nÃ¨re toutes les visualisations essentielles en une commande.

---

## ğŸ”§ DÃ©pannage

**"No JSON files found"**
- Lancer `Universal_Perf_Tester.py` d'abord pour gÃ©nÃ©rer les rÃ©sultats

**"No multi-threshold results"**
- Pour l'analyse de thresholds, tester avec plusieurs valeurs (0.4, 0.5, 0.6, etc.)

**Erreur d'import**
- VÃ©rifier que vous exÃ©cutez depuis le virtualenv : `.venv/bin/python`

---

## ğŸ“Š Outputs GÃ©nÃ©rÃ©s

Tous les rÃ©sultats sont sauvegardÃ©s dans `outputs/` :

**Visualisations PNG:**
- Performance globale et par classe
- Matrices de confusion
- Courbes par distance/type
- Impact des thresholds

**Rapports Texte:**
- `performance_summary.txt` - MÃ©triques dÃ©taillÃ©es
- `dataset_summary.txt` - Stats du dataset
- `threshold_recommendations.txt` - Thresholds optimaux

**DonnÃ©es JSON:**
- RÃ©sultats bruts pour post-traitement

**Pages HTML:**
- `audio_examples/index.html` - Exemples audio interactifs

---

**GÃ©nÃ¨re :**
- Spectre de difficultÃ© de dÃ©tection (trÃ¨s loin â†’ trÃ¨s proche)
- Courbes thÃ©oriques de performance vs distance
- Table d'analyse par catÃ©gorie de distance
- Distribution des Ã©chantillons par difficultÃ©

**Usage :**
```bash
python performance_by_distance.py
```

**Note :** Ce script gÃ©nÃ¨re des courbes thÃ©oriques. Pour des performances rÃ©elles par catÃ©gorie, il faudrait Ã©valuer les modÃ¨les sur un test set avec labels de catÃ©gorie SNR.

### 5. `run_all_visualizations.py`
Lance tous les scripts de visualisation en une seule commande.

**Usage :**
```bash
python run_all_visualizations.py
```

## Structure des sorties

Toutes les visualisations sont sauvegardÃ©es dans le dossier `outputs/` :

```
outputs/
â”œâ”€â”€ dataset_distribution.png
â”œâ”€â”€ snr_distribution.png
â”œâ”€â”€ audio_examples.png
â”œâ”€â”€ dataset_statistics.json
â”œâ”€â”€ training_curves.png
â”œâ”€â”€ confusion_matrices.png
â”œâ”€â”€ metrics_comparison.png
â”œâ”€â”€ performance_table.csv
â”œâ”€â”€ snr_performance.png
â”œâ”€â”€ augmentation_composition.png
â”œâ”€â”€ dataset_evolution.png
â”œâ”€â”€ difficulty_spectrum.png
â”œâ”€â”€ performance_vs_distance.png
â””â”€â”€ distance_analysis.csv
```

## DÃ©pendances

Les scripts nÃ©cessitent les bibliothÃ¨ques suivantes :
- matplotlib
- seaborn
- numpy
- pandas
- librosa
- scikit-learn

Ces dÃ©pendances sont normalement dÃ©jÃ  installÃ©es avec le projet principal.

## Ordre d'exÃ©cution recommandÃ©

1. **AprÃ¨s gÃ©nÃ©ration du dataset :** `dataset_analysis.py`
2. **AprÃ¨s entraÃ®nement des modÃ¨les :** `model_performance.py`
3. **Pour analyse complÃ¨te :** `augmentation_impact.py`
4. **Pour tout gÃ©nÃ©rer :** `run_all_visualizations.py`

## Notes

- Les scripts utilisent le fichier `config.py` Ã  la racine du projet pour les chemins
- Si un fichier de donnÃ©es est manquant, le script affiche un avertissement mais continue
- Les graphiques sont sauvegardÃ©s en haute rÃ©solution (300 DPI) pour publication
- Format des sorties : PNG pour les images, JSON/CSV pour les donnÃ©es

## Exemple de workflow complet

```bash
# 1. GÃ©nÃ©rer le dataset et entraÃ®ner les modÃ¨les
cd "../0 - DADS dataset extraction"
python master_setup.py --complete --max-per-class 500 --augment

cd "../1 - Preprocessing and Features Extraction"
python Mel_Preprocess_and_Feature_Extract.py
python MFCC_Preprocess_and_Feature_Extract.py

cd "../2 - Model Training"
python CNN_Trainer.py
python RNN_Trainer.py
python CRNN_Trainer.py

# 2. GÃ©nÃ©rer toutes les visualisations
cd "../6 - Visualization"
python run_all_visualizations.py
```

## Personnalisation

Pour personnaliser les visualisations, modifiez les paramÃ¨tres au dÃ©but de chaque script :
- Style matplotlib/seaborn
- Tailles des figures
- Couleurs
- Polices

## Support

En cas de problÃ¨me, vÃ©rifiez que :
1. Le dataset est bien gÃ©nÃ©rÃ© dans `0 - DADS dataset extraction/`
2. Les modÃ¨les sont entraÃ®nÃ©s et les rÃ©sultats sont dans `results/`
3. Les chemins dans `config.py` sont corrects
4. Toutes les dÃ©pendances sont installÃ©es
