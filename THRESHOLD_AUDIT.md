# Audit Syst√®me de Threshold - Architecture Actuelle et Propositions
**Date**: 2026-01-10

---

## üîç √âTAT DES LIEUX - Architecture Actuelle

### 1. **Sources de Threshold (3 emplacements diff√©rents!)**

#### A. `config.py` - Thresholds Hardcod√©s
```python
MODEL_THRESHOLDS = {
    "CNN": 0.61,
    "RNN": 0.01,  # ‚ö†Ô∏è Valeur suspecte!
    "CRNN": 0.70,
    "Attention_CRNN": 0.77,
}
```

**Probl√®mes**:
- ‚ùå Valeurs hardcod√©es (pas automatiques)
- ‚ùå RNN threshold = 0.01 semble erron√© (probablement jamais mis √† jour)
- ‚ùå Pas de m√©tadonn√©es (quand/comment calcul√©s?)
- ‚ùå Pas de versionning

#### B. Fichiers `*_threshold_optimization.json` (par mod√®le)
**Localisation**: `results/performance/cnn_threshold_optimization.json`

**Contenu**:
```json
{
  "model": "CNN",
  "validation_samples": 2000,
  "threshold_range": {"min": 0.3, "max": 0.7, "step": 0.05},
  "all_results": [...],  // Tous les points test√©s
  "best_threshold": 0.45,
  "best_metrics": {
    "threshold": 0.45,
    "accuracy": 0.6475,
    "precision": 0.597,
    "recall": 0.911,
    "f1_score": 0.721,
    "tp": 911, "tn": 384, "fp": 616, "fn": 89
  }
}
```

**Avantages**: ‚úÖ M√©tadonn√©es compl√®tes, historique des tests
**Probl√®mes**: 
- ‚ùå Pas charg√© automatiquement par les scripts
- ‚ùå Un fichier par mod√®le (pas centralis√©)
- ‚ùå Optimise SEULEMENT F1-score (crit√®re unique)

#### C. `threshold_recommendations.json` (visualisation)
**Localisation**: `6 - Visualization/outputs/threshold_recommendations.json`

**Contenu**:
```json
{
  "UNKNOWN_UNKNOWN": {  // ‚ö†Ô∏è Donn√©es invalides!
    "best_f1_threshold": 0.5,
    "best_f1_value": 0,
    "best_acc_threshold": 0.5,
    "balanced_threshold": 0.5
  }
}
```

**Probl√®mes**:
- ‚ùå Fichier corrompu (UNKNOWN_UNKNOWN au lieu des mod√®les)
- ‚ùå Pas utilis√© par les calculateurs de performance
- ‚ùå G√©n√©r√© par visualisation mais pas par le training

---

### 2. **Scripts d'Optimisation (2 versions redondantes)**

#### A. `optimize_threshold.py` - Version Simple
**Approche**: Grid search avec pas fixe (0.05)

**Workflow**:
```
1. Charger mod√®le
2. Charger MEL_VAL_DATA (validation set)
3. Tester thresholds de 0.3 √† 0.7 par pas de 0.05
4. Calculer m√©triques pour chaque threshold
5. S√©lectionner best_threshold par F1-score max
6. Sauvegarder dans {model}_threshold_optimization.json
```

**Crit√®re d'optimisation**: **F1-score UNIQUEMENT**

**Probl√®mes**:
- ‚ùå Un seul crit√®re (F1)
- ‚ùå Pas de consid√©ration des faux n√©gatifs vs faux positifs
- ‚ùå Pas d'√©quilibrage drone/ambient
- ‚ùå Grid search rigide (peut manquer l'optimal entre deux points)

#### B. `optimize_threshold_advanced.py` - Version Avanc√©e
**Approche**: Grid search + interpolation cubique + optimisation

**Workflow**:
```
1. Grid search initial (comme version simple)
2. Identifier r√©gion prometteuse (meilleur F1 ¬± 1 step)
3. Interpolation cubique de la courbe F1(threshold)
4. Optimisation par scipy.minimize_scalar dans la r√©gion
5. Affiner le threshold optimal (pr√©cision ~0.001)
```

**Crit√®re d'optimisation**: **F1-score UNIQUEMENT** (m√™me probl√®me!)

**Avantages**:
- ‚úÖ Plus pr√©cis (interpolation)
- ‚úÖ Trouve l'optimal r√©el entre les points du grid

**Probl√®mes**:
- ‚ùå TOUJOURS un seul crit√®re (F1)
- ‚ùå Complexit√© ajout√©e pour gain marginal
- ‚ùå Pas utilis√© dans le pipeline automatique

---

### 3. **Utilisation des Thresholds**

#### Dans `Universal_Perf_Tester.py`:
```python
# Ligne 173-178: R√©solution du threshold
default_threshold = 0.5
try:
    default_threshold = config.MODEL_THRESHOLDS_NORMALIZED.get(args.model.upper(), 
                                                                config.MODEL_THRESHOLDS.get(args.model, 0.5))
except Exception:
    default_threshold = config.MODEL_THRESHOLDS.get(args.model, 0.5)

resolved_threshold = args.threshold if args.threshold is not None else default_threshold
```

**Comportement actuel**:
1. Si `--threshold` fourni en argument ‚Üí utilise cette valeur
2. Sinon ‚Üí utilise `config.MODEL_THRESHOLDS[model]`
3. Fallback ‚Üí 0.5

**Probl√®mes**:
- ‚ùå Ne charge PAS les fichiers `*_threshold_optimization.json`
- ‚ùå N√©cessite mise √† jour manuelle de `config.py`
- ‚ùå Pas de workflow automatique apr√®s training

---

### 4. **Workflow Actuel (Cass√©)**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. TRAINING                                                 ‚îÇ
‚îÇ    python CNN_Trainer.py ‚Üí sauvegarde mod√®le                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. OPTIMIZATION (MANUEL - rarement fait!)                   ‚îÇ
‚îÇ    python optimize_threshold.py --model CNN                 ‚îÇ
‚îÇ    ‚Üí g√©n√®re cnn_threshold_optimization.json                 ‚îÇ
‚îÇ    ‚Üí best_threshold = 0.45                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. MISE √Ä JOUR CONFIG (MANUEL!)                             ‚îÇ
‚îÇ    √âditer config.py √† la main:                              ‚îÇ
‚îÇ    MODEL_THRESHOLDS["CNN"] = 0.45  # ‚Üê Jamais fait!         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. PERFORMANCE CALCULATION                                  ‚îÇ
‚îÇ    python Universal_Perf_Tester.py --model CNN              ‚îÇ
‚îÇ    ‚Üí utilise config.MODEL_THRESHOLDS["CNN"] = 0.61 (ancien!)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**RUPTURE**: Les thresholds optimis√©s ne sont jamais utilis√©s!

---

## üî¥ PROBL√àMES CRITIQUES IDENTIFI√âS

### Priority 1: **Crit√®re d'Optimisation Trop Simple**

**Actuel**: Maximise **F1-score uniquement**

**Probl√®me**: F1 = moyenne harmonique de Precision et Recall
- Donne m√™me poids aux FP (faux positifs) et FN (faux n√©gatifs)
- **Pas adapt√©** si co√ªts FN >> FP ou vice versa

**Besoin utilisateur** (d'apr√®s votre description):
> "maximiser l'efficacit√© avec des crit√®res d'importances diff√©rentes (du plus n√©cessaire au moins n√©cessaire) allant de l'√©cart faible entre la d√©tection de drone et de non drones, le taux de faux n√©gatif, la pr√©cision de d√©tection de drones, F1, etc..."

**Traduction**:
1. **√âcart faible entre d√©tection drone/non-drone** ‚Üí Balance precision_drone ‚âà precision_ambient (NPV)
2. **Taux de faux n√©gatifs bas** ‚Üí Recall √©lev√© (minimize FN)
3. **Pr√©cision de d√©tection de drones** ‚Üí Precision √©lev√©e (minimize FP)
4. **F1** ‚Üí Crit√®re composite (dernier recours)

**Ordre de priorit√© sugg√©r√©**:
```
1. HARD CONSTRAINTS (must satisfy):
   - Recall > 0.90 (max 10% FN - drones non d√©tect√©s)
   - Precision_drone > 0.70 (max 30% FP - fausses alarmes acceptables)

2. OPTIMIZATION TARGET (maximize):
   - Balanced score = min(Precision_drone, NPV_ambient)
   - ‚Üí Force √©quilibre entre classes
   
3. TIE-BREAKER (si √©galit√©):
   - F1-score max
```

---

### Priority 2: **Pas de Workflow Automatique**

**Probl√®mes**:
1. ‚ùå Optimisation threshold **manuelle** (jamais faite apr√®s training)
2. ‚ùå Mise √† jour config.py **manuelle**
3. ‚ùå Fichiers optimization JSONs **ignor√©s** par calculateurs
4. ‚ùå `config.MODEL_THRESHOLDS` **obsol√®tes**

**Exemple concret**:
```python
# config.py (ligne 195)
"RNN": 0.01  # ‚Üê WTF? Threshold √† 1%?
             # Probablement jamais optimis√© apr√®s le premier test
```

**Cons√©quence**: RNN va classifier TOUT comme drone (prob > 0.01)
‚Üí Pr√©cision catastrophique, FP √©normes

---

### Priority 3: **Fichier de Stockage Incoh√©rent**

**Actuel**: 3 fichiers diff√©rents avec formats diff√©rents
1. `config.py` ‚Üí Python dict hardcod√©
2. `results/performance/{model}_threshold_optimization.json` ‚Üí D√©tails complets
3. `6 - Visualization/outputs/threshold_recommendations.json` ‚Üí Corrompu

**Besoin**: **UN SEUL fichier centralis√©** chargeable en hot-plug

**Format sugg√©r√©**: `results/calibrated_thresholds.json`
```json
{
  "version": "1.0",
  "calibration_date": "2026-01-10T22:00:00",
  "calibration_mode": "balanced_precision",
  "models": {
    "CNN": {
      "threshold": 0.61,
      "validation_set": "val_2000_samples",
      "metrics_at_threshold": {
        "accuracy": 0.6475,
        "precision_drone": 0.597,
        "precision_ambient": 0.850,  // NPV
        "recall": 0.911,
        "specificity": 0.384,
        "f1_score": 0.721,
        "balanced_precision": 0.597  // min(prec_drone, NPV)
      },
      "constraints_met": {
        "min_recall_0.90": true,
        "min_precision_drone_0.70": false  // ‚ö†Ô∏è
      },
      "optimization_history": {
        "tested_range": [0.3, 0.7],
        "best_f1_threshold": 0.45,
        "best_balanced_threshold": 0.61,
        "recommended": 0.61
      }
    },
    "RNN": { ... },
    "CRNN": { ... },
    "Attention_CRNN": { ... }
  }
}
```

---

### Priority 4: **Calcul Bas√© sur VAL Uniquement**

**Actuel**: Optimisation sur **validation set** uniquement

**Probl√®me**: Risque d'overfitting au validation set

**Workflow recommand√©**:
```
1. G√©n√©rer pr√©dictions sur VAL (sans labels r√©v√©l√©s au mod√®le)
2. Optimiser threshold sur VAL
3. Tester threshold optimal sur TEST (ind√©pendant)
4. Si d√©gradation > 5% ‚Üí threshold trop overfitt√©
5. Calculer m√©triques finales sur TRAIN, VAL, TEST avec threshold optimal
```

**Note**: Vous mentionnez:
> "Le calcul de threshold doit se baser sur des pr√©dictions pr√©calcul√©es sur test et val"

‚Üí **Attention**: Optimiser sur TEST est du **data leakage**!
‚Üí **Solution**: Utiliser TEST seulement pour validation finale

---

## ‚úÖ PROPOSITIONS D'AM√âLIORATION

### Proposition 1: **Optimisation Multi-Crit√®res Hi√©rarchique**

**Nouveau script**: `calibrate_thresholds_v2.py`

**Crit√®res par ordre de priorit√©**:

#### **Tier 1: HARD CONSTRAINTS (must satisfy)**
```python
CONSTRAINTS = {
    'min_recall': 0.90,              # Max 10% FN (drones manqu√©s)
    'min_precision_drone': 0.70,     # Max 30% FP (fausses alarmes)
    'min_precision_ambient': 0.85,   # Max 15% erreurs ambient (NPV)
}
```

#### **Tier 2: OPTIMIZATION TARGET**
```python
def balanced_precision_score(precision_drone, precision_ambient):
    """
    Score √©quilibr√©: favorise √©quilibre entre classes.
    Pire classe domine (comme F-beta mais pour les deux classes).
    """
    return min(precision_drone, precision_ambient)
    # Alternative: harmonic mean = 2 / (1/p_drone + 1/p_ambient)
```

#### **Tier 3: TIE-BREAKERS**
```python
if balanced_precision_equal:
    # 1. Favoriser F1 plus √©lev√©
    # 2. Si √©galit√© F1, favoriser recall plus √©lev√© (moins de FN)
    # 3. Si √©galit√© recall, favoriser threshold plus bas (plus permissif)
```

**Pseudo-code**:
```python
def optimize_threshold_hierarchical(predictions_val, labels_val):
    """
    Optimise threshold avec crit√®res hi√©rarchiques.
    """
    candidates = []
    
    # Tester range de thresholds
    for threshold in np.arange(0.05, 0.95, 0.01):
        metrics = calculate_metrics(predictions_val, labels_val, threshold)
        
        # Tier 1: V√©rifier contraintes DURES
        if (metrics['recall'] >= 0.90 and
            metrics['precision_drone'] >= 0.70 and
            metrics['precision_ambient'] >= 0.85):
            
            # Tier 2: Calculer score d'optimisation
            metrics['balanced_precision'] = min(metrics['precision_drone'], 
                                                 metrics['precision_ambient'])
            candidates.append(metrics)
    
    if not candidates:
        # Aucun threshold ne satisfait les contraintes
        # ‚Üí Relax constraints progressivement
        print("‚ö†Ô∏è Aucun threshold satisfait les contraintes dures!")
        print("   Relaxation des contraintes...")
        return optimize_with_relaxed_constraints(...)
    
    # Tier 2: Trier par balanced_precision (desc)
    candidates.sort(key=lambda x: x['balanced_precision'], reverse=True)
    best = candidates[0]
    
    # Tier 3: Si plusieurs √©gaux, d√©partager par F1
    ties = [c for c in candidates if c['balanced_precision'] == best['balanced_precision']]
    if len(ties) > 1:
        best = max(ties, key=lambda x: (x['f1_score'], x['recall'], -x['threshold']))
    
    return best['threshold'], best
```

---

### Proposition 2: **Workflow Automatique**

**Int√©gration dans le pipeline**:

```bash
# 1. TRAINING (inchang√©)
python CNN_Trainer.py
# ‚Üí sauvegarde saved_models/CNN_model.h5

# 2. AUTO-CALIBRATION (nouveau, automatique!)
python calibrate_thresholds_v2.py --model CNN --auto-save
# ‚Üí g√©n√®re results/calibrated_thresholds.json
# ‚Üí met √† jour avec threshold optimal

# 3. PERFORMANCE CALCULATION (modifi√©)
python Universal_Perf_Tester.py --model CNN --use-calibrated
# ‚Üí charge threshold depuis calibrated_thresholds.json
# ‚Üí calcule m√©triques sur train/val/test avec threshold optimal
```

**Modifications requises**:

#### A. `Universal_Perf_Tester.py` - Charger thresholds calibr√©s
```python
def load_calibrated_threshold(model_name):
    """Charge threshold depuis calibrated_thresholds.json"""
    calib_file = config.CALIBRATION_FILE_PATH  # results/calibrated_thresholds.json
    
    if not calib_file.exists():
        print(f"‚ö†Ô∏è Pas de calibration trouv√©e: {calib_file}")
        print(f"   Utilisation threshold par d√©faut depuis config.py")
        return config.MODEL_THRESHOLDS.get(model_name, 0.5)
    
    with open(calib_file, 'r') as f:
        calib_data = json.load(f)
    
    model_data = calib_data.get('models', {}).get(model_name.upper())
    if not model_data:
        print(f"‚ö†Ô∏è Mod√®le {model_name} non trouv√© dans calibration")
        return config.MODEL_THRESHOLDS.get(model_name, 0.5)
    
    threshold = model_data['threshold']
    print(f"‚úì Threshold calibr√© charg√©: {threshold:.4f}")
    print(f"  Calibration date: {calib_data['calibration_date']}")
    print(f"  Mode: {calib_data['calibration_mode']}")
    return threshold

# Dans main():
if args.use_calibrated:
    threshold = load_calibrated_threshold(args.model)
else:
    threshold = args.threshold or config.MODEL_THRESHOLDS.get(args.model, 0.5)
```

#### B. `calibrate_thresholds_v2.py` - Script principal
```python
#!/usr/bin/env python3
"""
Threshold Calibration v2 - Multi-Criteria Hierarchical Optimization

Optimise les thresholds en tenant compte de:
1. Contraintes dures (min recall, min precision)
2. Score d'optimisation (balanced precision)
3. Tie-breakers (F1, recall, threshold)

Usage:
    # Calibrer un mod√®le
    python calibrate_thresholds_v2.py --model CNN
    
    # Calibrer tous les mod√®les
    python calibrate_thresholds_v2.py --all-models
    
    # Avec sauvegarde automatique
    python calibrate_thresholds_v2.py --all-models --auto-save
"""
```

---

### Proposition 3: **Hot-Plug Capabilities**

**Besoin**: Modifier thresholds sans re-run du code

**Solution**: Watcher de fichier avec reloading automatique

```python
# Dans config.py
import json
from pathlib import Path

class ThresholdManager:
    """Gestionnaire de thresholds avec hot-reload."""
    
    def __init__(self, calib_file_path):
        self.calib_file = Path(calib_file_path)
        self._cache = {}
        self._last_mtime = None
        self._load_if_changed()
    
    def _load_if_changed(self):
        """Recharge si fichier modifi√©."""
        if not self.calib_file.exists():
            return
        
        mtime = self.calib_file.stat().st_mtime
        if mtime != self._last_mtime:
            with open(self.calib_file, 'r') as f:
                data = json.load(f)
            self._cache = {m: d['threshold'] for m, d in data.get('models', {}).items()}
            self._last_mtime = mtime
            print(f"üîÑ Thresholds reloaded from {self.calib_file}")
    
    def get_threshold(self, model_name, default=0.5):
        """R√©cup√®re threshold avec auto-reload."""
        self._load_if_changed()  # Check for updates
        return self._cache.get(model_name.upper(), default)

# Instance globale
_threshold_manager = ThresholdManager(CALIBRATION_FILE_PATH)

def get_model_threshold(model_name):
    """API publique pour r√©cup√©rer threshold."""
    return _threshold_manager.get_threshold(model_name)
```

**Usage**:
```python
# Au lieu de:
threshold = config.MODEL_THRESHOLDS['CNN']

# Utiliser:
threshold = config.get_model_threshold('CNN')
# ‚Üí Recharge auto si calibrated_thresholds.json modifi√©
```

---

### Proposition 4: **Validation sur TEST**

**Workflow 2-phase**:

#### Phase 1: Calibration sur VAL
```python
# 1. Charger pr√©dictions VAL (pr√©calcul√©es)
predictions_val = load_predictions(model, 'val')

# 2. Optimiser threshold sur VAL
optimal_threshold = optimize_threshold_hierarchical(predictions_val, labels_val)

print(f"Optimal threshold (VAL): {optimal_threshold}")
```

#### Phase 2: Validation sur TEST
```python
# 3. Appliquer threshold sur TEST (ind√©pendant)
predictions_test = load_predictions(model, 'test')
metrics_test = calculate_metrics(predictions_test, labels_test, optimal_threshold)

# 4. V√©rifier d√©gradation
degradation_f1 = metrics_val['f1'] - metrics_test['f1']
if degradation_f1 > 0.05:
    print("‚ö†Ô∏è D√©gradation significative sur TEST!")
    print(f"   VAL F1: {metrics_val['f1']:.4f}")
    print(f"   TEST F1: {metrics_test['f1']:.4f}")
    print(f"   Œî: {degradation_f1:.4f}")
    print("   ‚Üí Threshold peut √™tre overfitt√© au VAL set")

# 5. Calculer m√©triques finales sur TRAIN, VAL, TEST
for split in ['train', 'val', 'test']:
    predictions = load_predictions(model, split)
    metrics = calculate_metrics(predictions, labels, optimal_threshold)
    save_metrics(model, split, threshold, metrics)
```

---

### Proposition 5: **Pr√©dictions Pr√©calcul√©es**

**Motivation**: √âviter de recharger le mod√®le √† chaque optimisation

**Workflow**:

#### √âtape 1: G√©n√©rer pr√©dictions (une fois apr√®s training)
```bash
python generate_predictions.py --model CNN --splits train val test
# ‚Üí sauvegarde results/predictions/cnn_train_predictions.npz
# ‚Üí sauvegarde results/predictions/cnn_val_predictions.npz
# ‚Üí sauvegarde results/predictions/cnn_test_predictions.npz
```

**Format NPZ**:
```python
np.savez_compressed(
    'cnn_val_predictions.npz',
    filenames=filenames,     # List[str]
    labels=labels,           # np.array[int]
    probabilities=probs,     # np.array[float] - P(class=1)
    features_shape=(44, 173),
    model_version='1.0'
)
```

#### √âtape 2: Optimiser threshold (rapide, sans mod√®le)
```bash
python calibrate_thresholds_v2.py --model CNN --use-precalc
# ‚Üí charge cnn_val_predictions.npz (pas besoin du mod√®le!)
# ‚Üí optimise threshold rapidement
# ‚Üí valide sur cnn_test_predictions.npz
```

**Avantages**:
- ‚úÖ Pas besoin de charger TensorFlow
- ‚úÖ Optimisation 100x plus rapide
- ‚úÖ Peut tester plusieurs modes de calibration sans recharger mod√®le
- ‚úÖ Facilite exp√©rimentation

---

## üìã PLAN D'IMPL√âMENTATION RECOMMAND√â

### Phase 1: Fix Urgent (1-2h)
1. **Corriger RNN threshold dans config.py**
   - Lancer optimize_threshold.py pour RNN
   - Mettre √† jour config.py avec valeur correcte
   - V√©rifier CNN/CRNN/Attention aussi

2. **Cr√©er calibrated_thresholds.json centralis√©**
   - Format JSON structur√© (voir Proposition 3)
   - Copier meilleurs thresholds depuis optimization JSONs existants
   - Placer dans `results/calibrated_thresholds.json`

3. **Modifier Universal_Perf_Tester pour charger calibrated_thresholds.json**
   - Ajouter flag `--use-calibrated` (default True)
   - Fonction `load_calibrated_threshold()`
   - Fallback vers config.py si fichier absent

### Phase 2: Calibration Multi-Crit√®res (3-4h)
1. **Cr√©er calibrate_thresholds_v2.py**
   - Impl√©menter optimisation hi√©rarchique
   - Contraintes dures configurables
   - Score balanced precision
   - Tie-breakers (F1, recall)

2. **Tester sur tous les mod√®les**
   - Comparer r√©sultats vs optimize_threshold.py
   - Valider que contraintes sont satisfaites
   - V√©rifier m√©triques sur TEST

3. **G√©n√©rer calibrated_thresholds.json automatiquement**
   - Format complet avec m√©tadonn√©es
   - Historique d'optimisation
   - Date de calibration

### Phase 3: Pr√©dictions Pr√©calcul√©es (2-3h)
1. **Cr√©er generate_predictions.py**
   - G√©n√®re .npz pour train/val/test
   - Sauvegarde dans results/predictions/

2. **Modifier calibrate_thresholds_v2.py**
   - Option `--use-precalc` pour charger .npz
   - Beaucoup plus rapide

3. **Int√©grer dans pipeline**
   - run_full_pipeline.sh g√©n√®re pr√©dictions apr√®s training
   - Puis calibration automatique
   - Puis calcul performance

### Phase 4: Hot-Reload (1-2h)
1. **Cr√©er ThresholdManager dans config.py**
   - Watcher de fichier
   - Auto-reload si modifi√©

2. **API publique get_model_threshold()**
   - Remplace acc√®s direct √† MODEL_THRESHOLDS
   - Check mtime √† chaque appel

### Phase 5: Documentation et Tests (1-2h)
1. **Documentation compl√®te**
   - README pour calibration
   - Exemples d'usage
   - Explication crit√®res

2. **Tests de validation**
   - V√©rifier contraintes satisfaites
   - Comparer VAL vs TEST
   - V√©rifier backward compatibility

---

## ‚ö†Ô∏è POINTS D'ATTENTION

### 1. **Backward Compatibility**
- Garder `config.MODEL_THRESHOLDS` comme fallback
- Si `calibrated_thresholds.json` absent ‚Üí utiliser config.py
- Scripts anciens continuent de fonctionner

### 2. **Contraintes Trop Strictes**
- Si aucun threshold satisfait les contraintes ‚Üí relaxation progressive
- Alerter utilisateur
- Sugg√©rer de r√©-entra√Æner le mod√®le ou ajuster contraintes

### 3. **Overfitting au Validation Set**
- TOUJOURS valider sur TEST
- Alerter si d√©gradation > 5%
- Consid√©rer cross-validation si dataset petit

### 4. **Crit√®res Application-Specific**
- Contraintes actuelles (recall > 0.90, precision > 0.70) sont des **suggestions**
- √Ä adapter selon contexte d'utilisation:
  - **Surveillance critique**: favoriser recall (moins de FN)
  - **Alerte publique**: favoriser precision (moins de FP)

---

## üéØ QUESTIONS POUR L'UTILISATEUR

Avant impl√©mentation, clarifier:

1. **Ordre de priorit√© des crit√®res** - Confirmer:
   - Tier 1: Recall > 0.90, Precision_drone > 0.70, NPV_ambient > 0.85?
   - Tier 2: Balanced precision (min des deux)?
   - Tier 3: F1-score comme tie-breaker?

2. **Co√ªt relatif FN vs FP**:
   - FN (drone non d√©tect√©): Quel impact? Critique ou acceptable?
   - FP (fausse alarme): G√™nant mais acceptable?
   - Ratio importance: FN = 2√óFP? 3√óFP?

3. **Usage des pr√©dictions pr√©calcul√©es**:
   - G√©n√©rer apr√®s chaque training?
   - Stocker dans results/predictions/?
   - Format NPZ ou JSON?

4. **Workflow automatique**:
   - Calibration automatique apr√®s training?
   - Int√©grer dans run_full_pipeline.sh?

5. **Hot-reload thresholds**:
   - N√©cessaire ou overkill?
   - Check mtime √† chaque appel (overhead)?

---

**Prochaine √©tape**: Attendre vos directives sur les priorit√©s et crit√®res avant impl√©mentation.
